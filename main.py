"""
main.py ‚Äî Full scraping pipeline:
CarTrade:
1. Fetch all live events
2. Filter insurance events
3. Fetch auction details for filtered ones
4. Download GJ images and metadata

CarDekho:
1. Fetch dashboard data
2. Filter insurance business data
3. Extract vehicle data and images
4. Download images and create metadata

Both:
5. Create zip archive
"""

import logging
from scraper.events_scraper import fetch_live_events, filter_insurance_events
from scraper.auction_details_scraper import fetch_auction_details
from scraper.download_gj_images import download_gj_images
from scraper.download_gj_images import create_date_zip
from scraper.cardekho_events_scraper import fetch_cardekho_dashboard_data, filter_insurance_business
from scraper.cardekho_vehicle_scraper import update_auction_paths_with_vehicles, download_cardekho_images
from dotenv import load_dotenv
import os

def main():
    load_dotenv()
    name = os.getenv("SCRAPER_NAME", "User")
    date = os.getenv("SCRAPE_START_DATE", "Unknown")

    logging.info("=" * 60)
    logging.info("üöÄ Web Scraping Pipeline Starting")
    logging.info("=" * 60)
    logging.info("üëã Hello %s, scraping started on %s", name, date)

    # ========== CarTrade Scraping ==========
    logging.info("")
    logging.info("=" * 60)
    logging.info("üìä CARTRADE SCRAPING")
    logging.info("=" * 60)

    raw_file = fetch_live_events()
    if not raw_file:
        logging.error("‚ùå Failed to fetch main events.")
        return False

    filtered_file, bid_file = filter_insurance_events(raw_file)
    if not bid_file:
        logging.error("‚ùå No bid paths to process.")
        return False

    logging.info("Starting detailed auction scraping...")
    fetch_auction_details()
    # After fetching auction details
    download_gj_images()

    logging.info("‚úÖ CarTrade scraping completed successfully!")

    # ========== CarDekho Scraping ==========
    logging.info("")
    logging.info("=" * 60)
    logging.info("üìä CARDEKHO SCRAPING")
    logging.info("=" * 60)

    cardekho_raw_file = fetch_cardekho_dashboard_data()
    if not cardekho_raw_file:
        logging.error("‚ùå Failed to fetch CarDekho dashboard data.")
        return False

    cardekho_filtered_file = filter_insurance_business(cardekho_raw_file)
    if not cardekho_filtered_file:
        logging.warning("‚ö†Ô∏è  No insurance business data found or filtering failed.")
        # Don't return False here, as this might be expected if structure is different
    else:
        logging.info("‚úÖ CarDekho filtering completed successfully!")
    
    # Step 3: Extract vehicle links from auction pages
    if cardekho_filtered_file:
        if not update_auction_paths_with_vehicles():
            logging.warning("‚ö†Ô∏è  Vehicle link extraction had errors, but continuing...")
        else:
            logging.info("‚úÖ CarDekho vehicle link extraction completed successfully!")
    
    # Step 4: Download images and create metadata
    logging.info("")
    logging.info("=" * 60)
    logging.info("üì• DOWNLOADING CARDEKHO IMAGES AND CREATING METADATA")
    logging.info("=" * 60)
    
    if not download_cardekho_images():
        logging.warning("‚ö†Ô∏è  CarDekho image download had errors, but continuing...")
    else:
        logging.info("‚úÖ CarDekho image download completed successfully!")
    
    # Step 5: Create zip archive (for both CarTrade and CarDekho)
    logging.info("")
    logging.info("=" * 60)
    logging.info("üì¶ CREATING ZIP ARCHIVE")
    logging.info("=" * 60)
    
    try:
        archive = create_date_zip(None)
        if archive:
            logging.info(f"‚úÖ Created archive: {archive}")
        else:
            logging.warning("‚ö†Ô∏è  create_date_zip did not create an archive")
    except Exception as e:
        logging.exception(f"üí• Error while creating date archive: {e}")

    logging.info("")
    logging.info("=" * 60)
    logging.info("üéØ All steps completed successfully!")
    logging.info("=" * 60)
    
    return True

if __name__ == "__main__":
    main()
